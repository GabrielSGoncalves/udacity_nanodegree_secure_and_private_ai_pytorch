{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data',train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',train=False, transform=transforms.ToTensor(),download = True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"A Neural Network with a hidden layer\"\"\"\n",
    "    def __init__(self, input_size,hidden_size,output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.layer2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3936\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3363\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2376\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2413\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1739\n",
      "Epoch [1/5], Step [600/600], Loss: 0.2084\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1560\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1561\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0383\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1251\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0544\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0838\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0517\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0421\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0498\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0332\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0798\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1522\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0506\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0733\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0275\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0468\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0698\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0759\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0947\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0392\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0125\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0258\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0267\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0397\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.81 %\n"
     ]
    }
   ],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the original MNIST dataset in IDX format\n",
    "From https://medium.com/@mannasiladittya/converting-mnist-data-in-idx-format-to-python-numpy-array-5cb9126f99f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import struct as st\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the IDX file in readable binary mode.\n",
    "filename = {'images' : './data/MNIST_original_dataset/train-images-idx3-ubyte' ,\n",
    "            'labels' : './data/MNIST_original_dataset/train-labels-idx1-ubyte'}\n",
    "train_imagesfile = open(filename['images'],'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth Step : Read the dimensions of the Image data-set\n",
    "nImg = st.unpack('>I',train_imagesfile.read(4))[0] #num of images\n",
    "nR = st.unpack('>I',train_imagesfile.read(4))[0] #num of rows\n",
    "nC = st.unpack('>I',train_imagesfile.read(4))[0] #num of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-564cb3417b2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fifth Step : Declare Image NumPy array (Optional, not required in this case)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnImg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fifth Step : Declare Image NumPy array (Optional, not required in this case)\n",
    "images_array = np.zeros((nImg,nR,nC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c5d82cfa7b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sixth Step : Reading the Image data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnBytesTotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnImg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnR\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnC\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m#since each pixel data is 1 byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimages_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnBytesTotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_imagesfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnBytesTotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnImg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sixth Step : Reading the Image data\n",
    "nBytesTotal = nImg*nR*nC*1 #since each pixel data is 1 byte\n",
    "images_array = 255 - np.asarray(st.unpack('>'+'B'*nBytesTotal, train_imagesfile.read(nBytesTotal))).reshape((nImg,nR,nC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the Kaggle Dataset\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_train.iloc[:, 1:785]\n",
    "df_label = df_train.iloc[:, 0]\n",
    "\n",
    "df_features = df_train.iloc[:, 1:785]\n",
    "df_label = df_train.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keyur Paralkar Notebook on Kaggle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_data_label(fileName):\n",
    "    df = pd.read_csv(filepath_or_buffer=PATH+fileName)\n",
    "    if(fileName != 'test.csv'):\n",
    "        label = np.array(df['label'])\n",
    "        data = np.array(df[df.columns[1:]],dtype=np.float)\n",
    "        new_data = np.reshape(a=data,newshape=(data.shape[0],28,28))\n",
    "        return new_data, label\n",
    "    else:\n",
    "        data = np.array(df,dtype=np.float)\n",
    "        new_data = np.reshape(a=data,newshape=(data.shape[0],28,28))\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, trainLabel = return_data_label('train.csv')\n",
    "testData = return_data_label('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the dataset\n",
    "trainData = trainData / 255\n",
    "trainData = (trainData - 0.5)/0.5\n",
    "\n",
    "testData = testData / 255\n",
    "testData = (testData - 0.5)/0.5\n",
    "\n",
    "trainData = torch.from_numpy(trainData)\n",
    "testData = torch.from_numpy(testData)\n",
    "trainData, testData = trainData.type(torch.FloatTensor), testData.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainData.unsqueeze_(dim=1)\n",
    "testData = testData.unsqueeze_(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = torch.utils.data.TensorDataset(trainData,torch.from_numpy(trainLabel))\n",
    "trainDataLoader = torch.utils.data.DataLoader(trainDataset,batch_size=100,shuffle=False, num_workers=4)\n",
    "\n",
    "# testDataset = torch.utils.data.TensorDataset(testData)\n",
    "testDataLoader = torch.utils.data.DataLoader(testData,batch_size=100,shuffle=False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches == \n",
      " 42000\n",
      "Testing batches == \n",
      " 28000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training batches == \\n\",len(trainData))\n",
    "print(\"Testing batches == \\n\",len(testData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing no. of examples of each type\n",
    "def total_count(loader):\n",
    "    totalClassCount = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for batch_id,(images,labels) in enumerate(loader):\n",
    "        for label in labels:\n",
    "            totalClassCount[int(label)] += 1\n",
    "    return totalClassCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit class =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train Set')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "print(\"Digit class = \",classes)\n",
    "totalCount = total_count(trainDataLoader)\n",
    "\n",
    "fig0, ax0 = plt.subplots()\n",
    "ax0.barh(y=classes,width=totalCount)\n",
    "ax0.set_xlabel('# Examples')\n",
    "ax0.set_ylabel('# Digit Classes')\n",
    "ax0.set_title('Train Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2a3806b390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing single digit:\n",
    "temp = trainDataLoader.dataset[0][0].numpy()\n",
    "temp = np.reshape(a=temp,newshape=(temp.shape[1],temp.shape[2]))\n",
    "plt.imshow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating LeNet5 nn class module\n",
    "# conv2d => relu => maxpooling => conv2d => relu => maxpooling => fully connected layer(fc)1 \n",
    "#=> fc2 => softmax output\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        self.fc1Size = 0\n",
    "        self.toKnowMaxPoolSize= False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),1)\n",
    "        \n",
    "        if(self.toKnowMaxPoolSize == True):\n",
    "            self.fc1Size = x.size()\n",
    "            print(x.size())\n",
    "            return\n",
    "        #now lets reshape the matrix i.e. unrolling the matrix\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1024, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n1 = Net()\n",
    "#n1 = n1.cuda()\n",
    "print(n1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function for training our NNs\n",
    "def train_model(model,mode,decay,criterion,dataloader,optimizer,dictionary,num_epochs=30):\n",
    "    #mode = True means model.train() and False is model.eval()\n",
    "    #decay = True means decrease LR with no. of epochs \n",
    "    \n",
    "    totalLoss = []\n",
    "    totalLRs = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    LR = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if(decay == True):\n",
    "            for param in optimizer.param_groups:\n",
    "                LR = param['lr'] * (0.1**(epoch//7))\n",
    "                param['lr'] = LR\n",
    "            totalLRs.append(LR)\n",
    "            \n",
    "        print(\"Epoch = {}/{} \".format(epoch,num_epochs),end=\" \")\n",
    "        for batch_id,(image, label) in enumerate(dataloader):\n",
    "            if(mode == True):\n",
    "                optimizer.zero_grad()\n",
    "                image = torch.autograd.Variable(image)\n",
    "                label = torch.autograd.Variable(label)\n",
    "                #image = image.cuda()\n",
    "                #label = label.cuda()\n",
    "            else:\n",
    "                image = torch.autograd.Variable(image)\n",
    "                #image = image.cuda()\n",
    "\n",
    "            output = model.forward(image)\n",
    "            \n",
    "            if(mode == True):\n",
    "                loss = criterion(output,label)\n",
    "\n",
    "            _, predictated = torch.max(output.data,1)\n",
    "            \n",
    "            if(mode == True):\n",
    "                correct += (predictated == label.data).sum()\n",
    "                total += label.size(0)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            del image,label\n",
    "            \n",
    "        #torch.cuda.empty_cache()\n",
    "        #torch.empty_cache()\n",
    "        print(\"Loss = {:.5f}\".format(loss.data[0]))\n",
    "        totalLoss.append(loss.data[0])\n",
    "        \n",
    "    dictionary['totalLoss'] = totalLoss\n",
    "    dictionary['correct'] = correct\n",
    "    dictionary['totalSize'] = total\n",
    "    dictionary['totalLRs'] = totalLRs\n",
    "    \n",
    "    return model,dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward => loss => backward => upadte weights\n",
    "n1.toKnowMaxPoolSize = False   # To print the size of the last maxpool layer.\n",
    "optimizer = torch.optim.SGD(n1.parameters(),lr=0.1)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0/50  "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-31a7d2ac75b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-30eacaacfdf4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, mode, decay, criterion, dataloader, optimizer, dictionary, num_epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#torch.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss = {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtotalLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "#Let's first find correct Learning Rate using Learning decay.\n",
    "\n",
    "dictModel = {}\n",
    "n1, dictModel = train_model(model=n1,mode=True,decay=True,criterion=criterion,dataloader=trainDataLoader,optimizer=optimizer,dictionary=dictModel,num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frank Lemuchahary Notebook\n",
    "https://www.kaggle.com/franklemuchahary/mnist-digit-recognition-using-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch utility imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "#neural net imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = \"./data/\"\n",
    "train_df = pd.read_csv(input_folder_path+\"train.csv\")\n",
    "test_df = pd.read_csv(input_folder_path+\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['label'].values\n",
    "train_images = (train_df.iloc[:,1:].values).astype('float32')\n",
    "test_images = (test_df.iloc[:,:].values).astype('float32')\n",
    "\n",
    "#Training and Validation Split\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels,\n",
    "                                                                     stratify=train_labels, random_state=123,\n",
    "                                                                     test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28)\n",
    "val_images = val_images.reshape(val_images.shape[0], 28, 28)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train samples\n",
    "for i in range(6, 9):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(train_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.title(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test samples\n",
    "for i in range(6, 9):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(test_images[i].squeeze(), cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_images_tensor = torch.tensor(train_images)/255.0\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "\n",
    "#val\n",
    "val_images_tensor = torch.tensor(val_images)/255.0\n",
    "val_labels_tensor = torch.tensor(val_labels)\n",
    "val_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n",
    "\n",
    "#test\n",
    "test_images_tensor = torch.tensor(test_images)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor, batch_size=16, num_workers=2, shuffle=True)\n",
    "test_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.local/lib/python3.7/site-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    img_grid = make_grid(data[0:8,].unsqueeze(1), nrow=8)\n",
    "    img_target_labels = target[0:8,].numpy()\n",
    "    break\n",
    "    \n",
    "plt.imshow(img_grid.numpy().transpose((1,2,0)))\n",
    "plt.rcParams['figure.figsize'] = (10, 2)\n",
    "plt.title(img_target_labels, size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        )\n",
    "        \n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128*7*7, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_block(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_block): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_block): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=6272, out_features=128, bias=True)\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Dropout(p=0.5)\n",
       "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Dropout(p=0.5)\n",
       "    (9): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = Net()\n",
    "conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=conv_model.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    conv_model = conv_model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epoch):\n",
    "    conv_model.train()\n",
    "    exp_lr_scheduler.step()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.unsqueeze(1)\n",
    "        data, target = data, target\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = conv_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                num_epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "            \n",
    "def evaluate(data_loader):\n",
    "    conv_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in data_loader:\n",
    "        data = data.unsqueeze(1)\n",
    "        data, target = data, target\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = conv_model(data)\n",
    "        \n",
    "        loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "        \n",
    "    print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [1600/33600 (5%)]\tLoss: 0.629091\n",
      "Train Epoch: 0 [3200/33600 (10%)]\tLoss: 0.542279\n",
      "Train Epoch: 0 [4800/33600 (14%)]\tLoss: 0.623876\n",
      "Train Epoch: 0 [6400/33600 (19%)]\tLoss: 0.733621\n",
      "Train Epoch: 0 [8000/33600 (24%)]\tLoss: 0.929409\n",
      "Train Epoch: 0 [9600/33600 (29%)]\tLoss: 0.537026\n",
      "Train Epoch: 0 [11200/33600 (33%)]\tLoss: 0.794428\n",
      "Train Epoch: 0 [12800/33600 (38%)]\tLoss: 0.496555\n",
      "Train Epoch: 0 [14400/33600 (43%)]\tLoss: 0.575411\n",
      "Train Epoch: 0 [16000/33600 (48%)]\tLoss: 0.566879\n",
      "Train Epoch: 0 [17600/33600 (52%)]\tLoss: 1.100626\n",
      "Train Epoch: 0 [19200/33600 (57%)]\tLoss: 0.618602\n",
      "Train Epoch: 0 [20800/33600 (62%)]\tLoss: 0.716567\n",
      "Train Epoch: 0 [22400/33600 (67%)]\tLoss: 0.617985\n",
      "Train Epoch: 0 [24000/33600 (71%)]\tLoss: 1.106295\n",
      "Train Epoch: 0 [25600/33600 (76%)]\tLoss: 0.564202\n",
      "Train Epoch: 0 [27200/33600 (81%)]\tLoss: 0.673608\n",
      "Train Epoch: 0 [28800/33600 (86%)]\tLoss: 0.598456\n",
      "Train Epoch: 0 [30400/33600 (90%)]\tLoss: 0.621114\n",
      "Train Epoch: 0 [32000/33600 (95%)]\tLoss: 0.773041\n",
      "Train Epoch: 0 [33600/33600 (100%)]\tLoss: 0.503463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.3093, Val Accuracy: 7860/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 1 [1600/33600 (5%)]\tLoss: 0.369897\n",
      "Train Epoch: 1 [3200/33600 (10%)]\tLoss: 0.420205\n",
      "Train Epoch: 1 [4800/33600 (14%)]\tLoss: 0.820128\n",
      "Train Epoch: 1 [6400/33600 (19%)]\tLoss: 0.380925\n",
      "Train Epoch: 1 [8000/33600 (24%)]\tLoss: 0.600868\n",
      "Train Epoch: 1 [9600/33600 (29%)]\tLoss: 1.045408\n",
      "Train Epoch: 1 [11200/33600 (33%)]\tLoss: 0.778509\n",
      "Train Epoch: 1 [12800/33600 (38%)]\tLoss: 1.441444\n",
      "Train Epoch: 1 [14400/33600 (43%)]\tLoss: 0.577983\n",
      "Train Epoch: 1 [16000/33600 (48%)]\tLoss: 0.780622\n",
      "Train Epoch: 1 [17600/33600 (52%)]\tLoss: 0.578715\n",
      "Train Epoch: 1 [19200/33600 (57%)]\tLoss: 0.386868\n",
      "Train Epoch: 1 [20800/33600 (62%)]\tLoss: 0.965955\n",
      "Train Epoch: 1 [22400/33600 (67%)]\tLoss: 0.377912\n",
      "Train Epoch: 1 [24000/33600 (71%)]\tLoss: 0.731055\n",
      "Train Epoch: 1 [25600/33600 (76%)]\tLoss: 0.432986\n",
      "Train Epoch: 1 [27200/33600 (81%)]\tLoss: 0.771130\n",
      "Train Epoch: 1 [28800/33600 (86%)]\tLoss: 0.712663\n",
      "Train Epoch: 1 [30400/33600 (90%)]\tLoss: 0.494195\n",
      "Train Epoch: 1 [32000/33600 (95%)]\tLoss: 0.604850\n",
      "Train Epoch: 1 [33600/33600 (100%)]\tLoss: 0.563070\n",
      "\n",
      "Average Val Loss: 0.2902, Val Accuracy: 7878/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 2 [1600/33600 (5%)]\tLoss: 0.413762\n",
      "Train Epoch: 2 [3200/33600 (10%)]\tLoss: 1.588004\n",
      "Train Epoch: 2 [4800/33600 (14%)]\tLoss: 0.549573\n",
      "Train Epoch: 2 [6400/33600 (19%)]\tLoss: 0.735114\n",
      "Train Epoch: 2 [8000/33600 (24%)]\tLoss: 0.304288\n",
      "Train Epoch: 2 [9600/33600 (29%)]\tLoss: 0.894919\n",
      "Train Epoch: 2 [11200/33600 (33%)]\tLoss: 0.733907\n",
      "Train Epoch: 2 [12800/33600 (38%)]\tLoss: 1.249837\n",
      "Train Epoch: 2 [14400/33600 (43%)]\tLoss: 0.781375\n",
      "Train Epoch: 2 [16000/33600 (48%)]\tLoss: 0.536643\n",
      "Train Epoch: 2 [17600/33600 (52%)]\tLoss: 0.763847\n",
      "Train Epoch: 2 [19200/33600 (57%)]\tLoss: 0.563882\n",
      "Train Epoch: 2 [20800/33600 (62%)]\tLoss: 0.851826\n",
      "Train Epoch: 2 [22400/33600 (67%)]\tLoss: 0.860764\n",
      "Train Epoch: 2 [24000/33600 (71%)]\tLoss: 0.610983\n",
      "Train Epoch: 2 [25600/33600 (76%)]\tLoss: 0.851678\n",
      "Train Epoch: 2 [27200/33600 (81%)]\tLoss: 0.460220\n",
      "Train Epoch: 2 [28800/33600 (86%)]\tLoss: 0.720600\n",
      "Train Epoch: 2 [30400/33600 (90%)]\tLoss: 0.554245\n",
      "Train Epoch: 2 [32000/33600 (95%)]\tLoss: 0.604865\n",
      "Train Epoch: 2 [33600/33600 (100%)]\tLoss: 1.007019\n",
      "\n",
      "Average Val Loss: 0.2981, Val Accuracy: 7865/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 3 [1600/33600 (5%)]\tLoss: 1.062749\n",
      "Train Epoch: 3 [3200/33600 (10%)]\tLoss: 0.565264\n",
      "Train Epoch: 3 [4800/33600 (14%)]\tLoss: 0.907823\n",
      "Train Epoch: 3 [6400/33600 (19%)]\tLoss: 0.509622\n",
      "Train Epoch: 3 [8000/33600 (24%)]\tLoss: 0.355524\n",
      "Train Epoch: 3 [9600/33600 (29%)]\tLoss: 0.438858\n",
      "Train Epoch: 3 [11200/33600 (33%)]\tLoss: 0.493832\n",
      "Train Epoch: 3 [12800/33600 (38%)]\tLoss: 0.724638\n",
      "Train Epoch: 3 [14400/33600 (43%)]\tLoss: 0.687931\n",
      "Train Epoch: 3 [16000/33600 (48%)]\tLoss: 1.096579\n",
      "Train Epoch: 3 [17600/33600 (52%)]\tLoss: 0.871232\n",
      "Train Epoch: 3 [19200/33600 (57%)]\tLoss: 0.520003\n",
      "Train Epoch: 3 [20800/33600 (62%)]\tLoss: 0.632712\n",
      "Train Epoch: 3 [22400/33600 (67%)]\tLoss: 0.499096\n",
      "Train Epoch: 3 [24000/33600 (71%)]\tLoss: 0.837418\n",
      "Train Epoch: 3 [25600/33600 (76%)]\tLoss: 0.522027\n",
      "Train Epoch: 3 [27200/33600 (81%)]\tLoss: 0.395918\n",
      "Train Epoch: 3 [28800/33600 (86%)]\tLoss: 0.693939\n",
      "Train Epoch: 3 [30400/33600 (90%)]\tLoss: 0.864743\n",
      "Train Epoch: 3 [32000/33600 (95%)]\tLoss: 0.619345\n",
      "Train Epoch: 3 [33600/33600 (100%)]\tLoss: 0.892507\n",
      "\n",
      "Average Val Loss: 0.2955, Val Accuracy: 7854/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 4 [1600/33600 (5%)]\tLoss: 0.635983\n",
      "Train Epoch: 4 [3200/33600 (10%)]\tLoss: 0.562552\n",
      "Train Epoch: 4 [4800/33600 (14%)]\tLoss: 0.648733\n",
      "Train Epoch: 4 [6400/33600 (19%)]\tLoss: 0.498795\n",
      "Train Epoch: 4 [8000/33600 (24%)]\tLoss: 0.871458\n",
      "Train Epoch: 4 [9600/33600 (29%)]\tLoss: 0.637108\n",
      "Train Epoch: 4 [11200/33600 (33%)]\tLoss: 0.744258\n",
      "Train Epoch: 4 [12800/33600 (38%)]\tLoss: 0.517873\n",
      "Train Epoch: 4 [14400/33600 (43%)]\tLoss: 0.995502\n",
      "Train Epoch: 4 [16000/33600 (48%)]\tLoss: 0.651618\n",
      "Train Epoch: 4 [17600/33600 (52%)]\tLoss: 0.806709\n",
      "Train Epoch: 4 [19200/33600 (57%)]\tLoss: 0.700781\n",
      "Train Epoch: 4 [20800/33600 (62%)]\tLoss: 0.645578\n",
      "Train Epoch: 4 [22400/33600 (67%)]\tLoss: 0.747031\n",
      "Train Epoch: 4 [24000/33600 (71%)]\tLoss: 0.723061\n",
      "Train Epoch: 4 [25600/33600 (76%)]\tLoss: 0.740323\n",
      "Train Epoch: 4 [27200/33600 (81%)]\tLoss: 1.143310\n",
      "Train Epoch: 4 [28800/33600 (86%)]\tLoss: 0.303937\n",
      "Train Epoch: 4 [30400/33600 (90%)]\tLoss: 0.348215\n",
      "Train Epoch: 4 [32000/33600 (95%)]\tLoss: 1.111986\n",
      "Train Epoch: 4 [33600/33600 (100%)]\tLoss: 0.487305\n",
      "\n",
      "Average Val Loss: 0.2999, Val Accuracy: 7860/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 5 [1600/33600 (5%)]\tLoss: 0.690801\n",
      "Train Epoch: 5 [3200/33600 (10%)]\tLoss: 0.690655\n",
      "Train Epoch: 5 [4800/33600 (14%)]\tLoss: 0.760864\n",
      "Train Epoch: 5 [6400/33600 (19%)]\tLoss: 0.323486\n",
      "Train Epoch: 5 [8000/33600 (24%)]\tLoss: 0.969597\n",
      "Train Epoch: 5 [9600/33600 (29%)]\tLoss: 0.373320\n",
      "Train Epoch: 5 [11200/33600 (33%)]\tLoss: 0.919886\n",
      "Train Epoch: 5 [12800/33600 (38%)]\tLoss: 0.359330\n",
      "Train Epoch: 5 [14400/33600 (43%)]\tLoss: 0.400011\n",
      "Train Epoch: 5 [16000/33600 (48%)]\tLoss: 0.542025\n",
      "Train Epoch: 5 [17600/33600 (52%)]\tLoss: 0.778823\n",
      "Train Epoch: 5 [19200/33600 (57%)]\tLoss: 0.612783\n",
      "Train Epoch: 5 [20800/33600 (62%)]\tLoss: 0.442934\n",
      "Train Epoch: 5 [22400/33600 (67%)]\tLoss: 0.340317\n",
      "Train Epoch: 5 [24000/33600 (71%)]\tLoss: 0.673611\n",
      "Train Epoch: 5 [25600/33600 (76%)]\tLoss: 0.446317\n",
      "Train Epoch: 5 [27200/33600 (81%)]\tLoss: 0.983961\n",
      "Train Epoch: 5 [28800/33600 (86%)]\tLoss: 0.680373\n",
      "Train Epoch: 5 [30400/33600 (90%)]\tLoss: 0.397606\n",
      "Train Epoch: 5 [32000/33600 (95%)]\tLoss: 0.682151\n",
      "Train Epoch: 5 [33600/33600 (100%)]\tLoss: 0.603229\n",
      "\n",
      "Average Val Loss: 0.3058, Val Accuracy: 7853/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 6 [1600/33600 (5%)]\tLoss: 0.528384\n",
      "Train Epoch: 6 [3200/33600 (10%)]\tLoss: 0.592558\n",
      "Train Epoch: 6 [4800/33600 (14%)]\tLoss: 0.474633\n",
      "Train Epoch: 6 [6400/33600 (19%)]\tLoss: 0.513628\n",
      "Train Epoch: 6 [8000/33600 (24%)]\tLoss: 0.807452\n",
      "Train Epoch: 6 [9600/33600 (29%)]\tLoss: 0.715119\n",
      "Train Epoch: 6 [11200/33600 (33%)]\tLoss: 0.525776\n",
      "Train Epoch: 6 [12800/33600 (38%)]\tLoss: 1.191174\n",
      "Train Epoch: 6 [14400/33600 (43%)]\tLoss: 1.129105\n",
      "Train Epoch: 6 [16000/33600 (48%)]\tLoss: 0.497926\n",
      "Train Epoch: 6 [17600/33600 (52%)]\tLoss: 0.987547\n",
      "Train Epoch: 6 [19200/33600 (57%)]\tLoss: 0.684545\n",
      "Train Epoch: 6 [20800/33600 (62%)]\tLoss: 0.572970\n",
      "Train Epoch: 6 [22400/33600 (67%)]\tLoss: 0.723028\n",
      "Train Epoch: 6 [24000/33600 (71%)]\tLoss: 0.656416\n",
      "Train Epoch: 6 [25600/33600 (76%)]\tLoss: 0.406565\n",
      "Train Epoch: 6 [27200/33600 (81%)]\tLoss: 0.616568\n",
      "Train Epoch: 6 [28800/33600 (86%)]\tLoss: 0.573154\n",
      "Train Epoch: 6 [30400/33600 (90%)]\tLoss: 0.550386\n",
      "Train Epoch: 6 [32000/33600 (95%)]\tLoss: 0.539754\n",
      "Train Epoch: 6 [33600/33600 (100%)]\tLoss: 0.373025\n",
      "\n",
      "Average Val Loss: 0.2864, Val Accuracy: 7880/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 7 [1600/33600 (5%)]\tLoss: 0.389647\n",
      "Train Epoch: 7 [3200/33600 (10%)]\tLoss: 0.455927\n",
      "Train Epoch: 7 [4800/33600 (14%)]\tLoss: 0.524754\n",
      "Train Epoch: 7 [6400/33600 (19%)]\tLoss: 0.811924\n",
      "Train Epoch: 7 [8000/33600 (24%)]\tLoss: 0.715919\n",
      "Train Epoch: 7 [9600/33600 (29%)]\tLoss: 0.771717\n",
      "Train Epoch: 7 [11200/33600 (33%)]\tLoss: 0.749887\n",
      "Train Epoch: 7 [12800/33600 (38%)]\tLoss: 0.536203\n",
      "Train Epoch: 7 [14400/33600 (43%)]\tLoss: 0.825177\n",
      "Train Epoch: 7 [16000/33600 (48%)]\tLoss: 0.603332\n",
      "Train Epoch: 7 [17600/33600 (52%)]\tLoss: 0.819712\n",
      "Train Epoch: 7 [19200/33600 (57%)]\tLoss: 0.746319\n",
      "Train Epoch: 7 [20800/33600 (62%)]\tLoss: 0.305438\n",
      "Train Epoch: 7 [22400/33600 (67%)]\tLoss: 0.622166\n",
      "Train Epoch: 7 [24000/33600 (71%)]\tLoss: 0.555306\n",
      "Train Epoch: 7 [25600/33600 (76%)]\tLoss: 0.781783\n",
      "Train Epoch: 7 [27200/33600 (81%)]\tLoss: 0.839878\n",
      "Train Epoch: 7 [28800/33600 (86%)]\tLoss: 0.830514\n",
      "Train Epoch: 7 [30400/33600 (90%)]\tLoss: 0.640581\n",
      "Train Epoch: 7 [32000/33600 (95%)]\tLoss: 0.810414\n",
      "Train Epoch: 7 [33600/33600 (100%)]\tLoss: 0.640932\n",
      "\n",
      "Average Val Loss: 0.2912, Val Accuracy: 7878/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 8 [1600/33600 (5%)]\tLoss: 0.390077\n",
      "Train Epoch: 8 [3200/33600 (10%)]\tLoss: 0.833402\n",
      "Train Epoch: 8 [4800/33600 (14%)]\tLoss: 0.685804\n",
      "Train Epoch: 8 [6400/33600 (19%)]\tLoss: 0.920520\n",
      "Train Epoch: 8 [8000/33600 (24%)]\tLoss: 0.661036\n",
      "Train Epoch: 8 [9600/33600 (29%)]\tLoss: 0.622103\n",
      "Train Epoch: 8 [11200/33600 (33%)]\tLoss: 1.011427\n",
      "Train Epoch: 8 [12800/33600 (38%)]\tLoss: 0.923083\n",
      "Train Epoch: 8 [14400/33600 (43%)]\tLoss: 0.621154\n",
      "Train Epoch: 8 [16000/33600 (48%)]\tLoss: 0.487947\n",
      "Train Epoch: 8 [17600/33600 (52%)]\tLoss: 0.704954\n",
      "Train Epoch: 8 [19200/33600 (57%)]\tLoss: 1.029620\n",
      "Train Epoch: 8 [20800/33600 (62%)]\tLoss: 0.550924\n",
      "Train Epoch: 8 [22400/33600 (67%)]\tLoss: 0.596305\n",
      "Train Epoch: 8 [24000/33600 (71%)]\tLoss: 0.328868\n",
      "Train Epoch: 8 [25600/33600 (76%)]\tLoss: 0.532252\n",
      "Train Epoch: 8 [27200/33600 (81%)]\tLoss: 0.896075\n",
      "Train Epoch: 8 [28800/33600 (86%)]\tLoss: 0.558330\n",
      "Train Epoch: 8 [30400/33600 (90%)]\tLoss: 0.734030\n",
      "Train Epoch: 8 [32000/33600 (95%)]\tLoss: 0.455510\n",
      "Train Epoch: 8 [33600/33600 (100%)]\tLoss: 0.610384\n",
      "\n",
      "Average Val Loss: 0.2968, Val Accuracy: 7849/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 9 [1600/33600 (5%)]\tLoss: 0.903232\n",
      "Train Epoch: 9 [3200/33600 (10%)]\tLoss: 0.701627\n",
      "Train Epoch: 9 [4800/33600 (14%)]\tLoss: 0.714033\n",
      "Train Epoch: 9 [6400/33600 (19%)]\tLoss: 0.430887\n",
      "Train Epoch: 9 [8000/33600 (24%)]\tLoss: 1.082526\n",
      "Train Epoch: 9 [9600/33600 (29%)]\tLoss: 1.214971\n",
      "Train Epoch: 9 [11200/33600 (33%)]\tLoss: 0.755266\n",
      "Train Epoch: 9 [12800/33600 (38%)]\tLoss: 0.563995\n",
      "Train Epoch: 9 [14400/33600 (43%)]\tLoss: 0.869685\n",
      "Train Epoch: 9 [16000/33600 (48%)]\tLoss: 0.671930\n",
      "Train Epoch: 9 [17600/33600 (52%)]\tLoss: 0.436060\n",
      "Train Epoch: 9 [19200/33600 (57%)]\tLoss: 0.385128\n",
      "Train Epoch: 9 [20800/33600 (62%)]\tLoss: 0.560700\n",
      "Train Epoch: 9 [22400/33600 (67%)]\tLoss: 0.614055\n",
      "Train Epoch: 9 [24000/33600 (71%)]\tLoss: 1.065104\n",
      "Train Epoch: 9 [25600/33600 (76%)]\tLoss: 0.597444\n",
      "Train Epoch: 9 [27200/33600 (81%)]\tLoss: 1.262160\n",
      "Train Epoch: 9 [28800/33600 (86%)]\tLoss: 0.800520\n",
      "Train Epoch: 9 [30400/33600 (90%)]\tLoss: 0.489119\n",
      "Train Epoch: 9 [32000/33600 (95%)]\tLoss: 0.569419\n",
      "Train Epoch: 9 [33600/33600 (100%)]\tLoss: 0.653842\n",
      "\n",
      "Average Val Loss: 0.2961, Val Accuracy: 7842/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 10 [1600/33600 (5%)]\tLoss: 0.593764\n",
      "Train Epoch: 10 [3200/33600 (10%)]\tLoss: 0.446196\n",
      "Train Epoch: 10 [4800/33600 (14%)]\tLoss: 0.476772\n",
      "Train Epoch: 10 [6400/33600 (19%)]\tLoss: 0.424727\n",
      "Train Epoch: 10 [8000/33600 (24%)]\tLoss: 1.227830\n",
      "Train Epoch: 10 [9600/33600 (29%)]\tLoss: 0.650435\n",
      "Train Epoch: 10 [11200/33600 (33%)]\tLoss: 0.697482\n",
      "Train Epoch: 10 [12800/33600 (38%)]\tLoss: 0.560971\n",
      "Train Epoch: 10 [14400/33600 (43%)]\tLoss: 0.771208\n",
      "Train Epoch: 10 [16000/33600 (48%)]\tLoss: 0.575901\n",
      "Train Epoch: 10 [17600/33600 (52%)]\tLoss: 0.615136\n",
      "Train Epoch: 10 [19200/33600 (57%)]\tLoss: 0.940054\n",
      "Train Epoch: 10 [20800/33600 (62%)]\tLoss: 0.552422\n",
      "Train Epoch: 10 [22400/33600 (67%)]\tLoss: 0.737369\n",
      "Train Epoch: 10 [24000/33600 (71%)]\tLoss: 0.567384\n",
      "Train Epoch: 10 [25600/33600 (76%)]\tLoss: 0.629125\n",
      "Train Epoch: 10 [27200/33600 (81%)]\tLoss: 0.559146\n",
      "Train Epoch: 10 [28800/33600 (86%)]\tLoss: 1.146114\n",
      "Train Epoch: 10 [30400/33600 (90%)]\tLoss: 0.390364\n",
      "Train Epoch: 10 [32000/33600 (95%)]\tLoss: 0.695312\n",
      "Train Epoch: 10 [33600/33600 (100%)]\tLoss: 0.939950\n",
      "\n",
      "Average Val Loss: 0.2944, Val Accuracy: 7864/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 11 [1600/33600 (5%)]\tLoss: 0.733536\n",
      "Train Epoch: 11 [3200/33600 (10%)]\tLoss: 0.586242\n",
      "Train Epoch: 11 [4800/33600 (14%)]\tLoss: 1.284733\n",
      "Train Epoch: 11 [6400/33600 (19%)]\tLoss: 0.809491\n",
      "Train Epoch: 11 [8000/33600 (24%)]\tLoss: 0.469929\n",
      "Train Epoch: 11 [9600/33600 (29%)]\tLoss: 0.418863\n",
      "Train Epoch: 11 [11200/33600 (33%)]\tLoss: 0.839816\n",
      "Train Epoch: 11 [12800/33600 (38%)]\tLoss: 0.614770\n",
      "Train Epoch: 11 [14400/33600 (43%)]\tLoss: 0.560708\n",
      "Train Epoch: 11 [16000/33600 (48%)]\tLoss: 0.346373\n",
      "Train Epoch: 11 [17600/33600 (52%)]\tLoss: 0.947390\n",
      "Train Epoch: 11 [19200/33600 (57%)]\tLoss: 0.705004\n",
      "Train Epoch: 11 [20800/33600 (62%)]\tLoss: 0.296836\n",
      "Train Epoch: 11 [22400/33600 (67%)]\tLoss: 0.476673\n",
      "Train Epoch: 11 [24000/33600 (71%)]\tLoss: 0.755090\n",
      "Train Epoch: 11 [25600/33600 (76%)]\tLoss: 0.527336\n",
      "Train Epoch: 11 [27200/33600 (81%)]\tLoss: 0.749356\n",
      "Train Epoch: 11 [28800/33600 (86%)]\tLoss: 0.790512\n",
      "Train Epoch: 11 [30400/33600 (90%)]\tLoss: 0.739573\n",
      "Train Epoch: 11 [32000/33600 (95%)]\tLoss: 1.089778\n",
      "Train Epoch: 11 [33600/33600 (100%)]\tLoss: 0.340237\n",
      "\n",
      "Average Val Loss: 0.3005, Val Accuracy: 7857/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 12 [1600/33600 (5%)]\tLoss: 0.841575\n",
      "Train Epoch: 12 [3200/33600 (10%)]\tLoss: 1.052402\n",
      "Train Epoch: 12 [4800/33600 (14%)]\tLoss: 0.741752\n",
      "Train Epoch: 12 [6400/33600 (19%)]\tLoss: 0.802265\n",
      "Train Epoch: 12 [8000/33600 (24%)]\tLoss: 0.819350\n",
      "Train Epoch: 12 [9600/33600 (29%)]\tLoss: 0.655059\n",
      "Train Epoch: 12 [11200/33600 (33%)]\tLoss: 0.603734\n",
      "Train Epoch: 12 [12800/33600 (38%)]\tLoss: 0.796466\n",
      "Train Epoch: 12 [14400/33600 (43%)]\tLoss: 0.878060\n",
      "Train Epoch: 12 [16000/33600 (48%)]\tLoss: 0.684360\n",
      "Train Epoch: 12 [17600/33600 (52%)]\tLoss: 0.680091\n",
      "Train Epoch: 12 [19200/33600 (57%)]\tLoss: 0.783468\n",
      "Train Epoch: 12 [20800/33600 (62%)]\tLoss: 0.668972\n",
      "Train Epoch: 12 [22400/33600 (67%)]\tLoss: 0.861289\n",
      "Train Epoch: 12 [24000/33600 (71%)]\tLoss: 0.651223\n",
      "Train Epoch: 12 [25600/33600 (76%)]\tLoss: 0.828396\n",
      "Train Epoch: 12 [27200/33600 (81%)]\tLoss: 0.471823\n",
      "Train Epoch: 12 [28800/33600 (86%)]\tLoss: 0.609856\n",
      "Train Epoch: 12 [30400/33600 (90%)]\tLoss: 0.750800\n",
      "Train Epoch: 12 [32000/33600 (95%)]\tLoss: 0.661211\n",
      "Train Epoch: 12 [33600/33600 (100%)]\tLoss: 1.104068\n",
      "\n",
      "Average Val Loss: 0.2964, Val Accuracy: 7845/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 13 [1600/33600 (5%)]\tLoss: 0.473195\n",
      "Train Epoch: 13 [3200/33600 (10%)]\tLoss: 0.933320\n",
      "Train Epoch: 13 [4800/33600 (14%)]\tLoss: 0.647803\n",
      "Train Epoch: 13 [6400/33600 (19%)]\tLoss: 0.701187\n",
      "Train Epoch: 13 [8000/33600 (24%)]\tLoss: 0.618813\n",
      "Train Epoch: 13 [9600/33600 (29%)]\tLoss: 0.587729\n",
      "Train Epoch: 13 [11200/33600 (33%)]\tLoss: 0.381267\n",
      "Train Epoch: 13 [12800/33600 (38%)]\tLoss: 0.933310\n",
      "Train Epoch: 13 [14400/33600 (43%)]\tLoss: 0.660870\n",
      "Train Epoch: 13 [16000/33600 (48%)]\tLoss: 0.574726\n",
      "Train Epoch: 13 [17600/33600 (52%)]\tLoss: 1.036017\n",
      "Train Epoch: 13 [19200/33600 (57%)]\tLoss: 0.813657\n",
      "Train Epoch: 13 [20800/33600 (62%)]\tLoss: 0.435389\n",
      "Train Epoch: 13 [22400/33600 (67%)]\tLoss: 0.437992\n",
      "Train Epoch: 13 [24000/33600 (71%)]\tLoss: 0.405023\n",
      "Train Epoch: 13 [25600/33600 (76%)]\tLoss: 0.486602\n",
      "Train Epoch: 13 [27200/33600 (81%)]\tLoss: 0.343554\n",
      "Train Epoch: 13 [28800/33600 (86%)]\tLoss: 0.949636\n",
      "Train Epoch: 13 [30400/33600 (90%)]\tLoss: 0.610781\n",
      "Train Epoch: 13 [32000/33600 (95%)]\tLoss: 0.758948\n",
      "Train Epoch: 13 [33600/33600 (100%)]\tLoss: 0.922446\n",
      "\n",
      "Average Val Loss: 0.3033, Val Accuracy: 7865/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 14 [1600/33600 (5%)]\tLoss: 0.854904\n",
      "Train Epoch: 14 [3200/33600 (10%)]\tLoss: 0.664405\n",
      "Train Epoch: 14 [4800/33600 (14%)]\tLoss: 0.491459\n",
      "Train Epoch: 14 [6400/33600 (19%)]\tLoss: 0.422458\n",
      "Train Epoch: 14 [8000/33600 (24%)]\tLoss: 0.395769\n",
      "Train Epoch: 14 [9600/33600 (29%)]\tLoss: 0.559533\n",
      "Train Epoch: 14 [11200/33600 (33%)]\tLoss: 0.574250\n",
      "Train Epoch: 14 [12800/33600 (38%)]\tLoss: 0.842669\n",
      "Train Epoch: 14 [14400/33600 (43%)]\tLoss: 0.686139\n",
      "Train Epoch: 14 [16000/33600 (48%)]\tLoss: 0.679577\n",
      "Train Epoch: 14 [17600/33600 (52%)]\tLoss: 0.450830\n",
      "Train Epoch: 14 [19200/33600 (57%)]\tLoss: 0.660258\n",
      "Train Epoch: 14 [20800/33600 (62%)]\tLoss: 0.761643\n",
      "Train Epoch: 14 [22400/33600 (67%)]\tLoss: 0.688423\n",
      "Train Epoch: 14 [24000/33600 (71%)]\tLoss: 1.280443\n",
      "Train Epoch: 14 [25600/33600 (76%)]\tLoss: 0.603484\n",
      "Train Epoch: 14 [27200/33600 (81%)]\tLoss: 0.899153\n",
      "Train Epoch: 14 [28800/33600 (86%)]\tLoss: 0.928334\n",
      "Train Epoch: 14 [30400/33600 (90%)]\tLoss: 0.447768\n",
      "Train Epoch: 14 [32000/33600 (95%)]\tLoss: 0.521691\n",
      "Train Epoch: 14 [33600/33600 (100%)]\tLoss: 0.928709\n",
      "\n",
      "Average Val Loss: 0.2902, Val Accuracy: 7877/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 15 [1600/33600 (5%)]\tLoss: 0.805251\n",
      "Train Epoch: 15 [3200/33600 (10%)]\tLoss: 0.948176\n",
      "Train Epoch: 15 [4800/33600 (14%)]\tLoss: 0.673773\n",
      "Train Epoch: 15 [6400/33600 (19%)]\tLoss: 0.516972\n",
      "Train Epoch: 15 [8000/33600 (24%)]\tLoss: 0.470766\n",
      "Train Epoch: 15 [9600/33600 (29%)]\tLoss: 0.711976\n",
      "Train Epoch: 15 [11200/33600 (33%)]\tLoss: 0.547936\n",
      "Train Epoch: 15 [12800/33600 (38%)]\tLoss: 0.349816\n",
      "Train Epoch: 15 [14400/33600 (43%)]\tLoss: 0.557266\n",
      "Train Epoch: 15 [16000/33600 (48%)]\tLoss: 0.712333\n",
      "Train Epoch: 15 [17600/33600 (52%)]\tLoss: 0.436675\n",
      "Train Epoch: 15 [19200/33600 (57%)]\tLoss: 0.809568\n",
      "Train Epoch: 15 [20800/33600 (62%)]\tLoss: 0.493307\n",
      "Train Epoch: 15 [22400/33600 (67%)]\tLoss: 0.445311\n",
      "Train Epoch: 15 [24000/33600 (71%)]\tLoss: 0.298416\n",
      "Train Epoch: 15 [25600/33600 (76%)]\tLoss: 0.521900\n",
      "Train Epoch: 15 [27200/33600 (81%)]\tLoss: 0.590557\n",
      "Train Epoch: 15 [28800/33600 (86%)]\tLoss: 0.707357\n",
      "Train Epoch: 15 [30400/33600 (90%)]\tLoss: 1.029374\n",
      "Train Epoch: 15 [32000/33600 (95%)]\tLoss: 0.957185\n",
      "Train Epoch: 15 [33600/33600 (100%)]\tLoss: 0.527164\n",
      "\n",
      "Average Val Loss: 0.2993, Val Accuracy: 7855/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 16 [1600/33600 (5%)]\tLoss: 0.618675\n",
      "Train Epoch: 16 [3200/33600 (10%)]\tLoss: 0.580840\n",
      "Train Epoch: 16 [4800/33600 (14%)]\tLoss: 0.273465\n",
      "Train Epoch: 16 [6400/33600 (19%)]\tLoss: 0.550274\n",
      "Train Epoch: 16 [8000/33600 (24%)]\tLoss: 1.129582\n",
      "Train Epoch: 16 [9600/33600 (29%)]\tLoss: 0.778127\n",
      "Train Epoch: 16 [11200/33600 (33%)]\tLoss: 0.744241\n",
      "Train Epoch: 16 [12800/33600 (38%)]\tLoss: 0.433842\n",
      "Train Epoch: 16 [14400/33600 (43%)]\tLoss: 0.762529\n",
      "Train Epoch: 16 [16000/33600 (48%)]\tLoss: 0.839906\n",
      "Train Epoch: 16 [17600/33600 (52%)]\tLoss: 0.796200\n",
      "Train Epoch: 16 [19200/33600 (57%)]\tLoss: 0.997283\n",
      "Train Epoch: 16 [20800/33600 (62%)]\tLoss: 0.364356\n",
      "Train Epoch: 16 [22400/33600 (67%)]\tLoss: 0.604430\n",
      "Train Epoch: 16 [24000/33600 (71%)]\tLoss: 0.539738\n",
      "Train Epoch: 16 [25600/33600 (76%)]\tLoss: 1.619279\n",
      "Train Epoch: 16 [27200/33600 (81%)]\tLoss: 0.390755\n",
      "Train Epoch: 16 [28800/33600 (86%)]\tLoss: 1.125773\n",
      "Train Epoch: 16 [30400/33600 (90%)]\tLoss: 0.618888\n",
      "Train Epoch: 16 [32000/33600 (95%)]\tLoss: 0.646998\n",
      "Train Epoch: 16 [33600/33600 (100%)]\tLoss: 0.482495\n",
      "\n",
      "Average Val Loss: 0.2807, Val Accuracy: 7893/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 17 [1600/33600 (5%)]\tLoss: 0.679121\n",
      "Train Epoch: 17 [3200/33600 (10%)]\tLoss: 0.686811\n",
      "Train Epoch: 17 [4800/33600 (14%)]\tLoss: 0.814099\n",
      "Train Epoch: 17 [6400/33600 (19%)]\tLoss: 0.389722\n",
      "Train Epoch: 17 [8000/33600 (24%)]\tLoss: 0.662261\n",
      "Train Epoch: 17 [9600/33600 (29%)]\tLoss: 0.883126\n",
      "Train Epoch: 17 [11200/33600 (33%)]\tLoss: 0.951356\n",
      "Train Epoch: 17 [12800/33600 (38%)]\tLoss: 0.664239\n",
      "Train Epoch: 17 [14400/33600 (43%)]\tLoss: 0.428946\n",
      "Train Epoch: 17 [16000/33600 (48%)]\tLoss: 0.731561\n",
      "Train Epoch: 17 [17600/33600 (52%)]\tLoss: 0.518058\n",
      "Train Epoch: 17 [19200/33600 (57%)]\tLoss: 1.154922\n",
      "Train Epoch: 17 [20800/33600 (62%)]\tLoss: 0.783869\n",
      "Train Epoch: 17 [22400/33600 (67%)]\tLoss: 1.111514\n",
      "Train Epoch: 17 [24000/33600 (71%)]\tLoss: 0.811779\n",
      "Train Epoch: 17 [25600/33600 (76%)]\tLoss: 0.581746\n",
      "Train Epoch: 17 [27200/33600 (81%)]\tLoss: 1.212809\n",
      "Train Epoch: 17 [28800/33600 (86%)]\tLoss: 0.979742\n",
      "Train Epoch: 17 [30400/33600 (90%)]\tLoss: 0.445459\n",
      "Train Epoch: 17 [32000/33600 (95%)]\tLoss: 0.614793\n",
      "Train Epoch: 17 [33600/33600 (100%)]\tLoss: 0.390222\n",
      "\n",
      "Average Val Loss: 0.3076, Val Accuracy: 7856/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 18 [1600/33600 (5%)]\tLoss: 0.407148\n",
      "Train Epoch: 18 [3200/33600 (10%)]\tLoss: 0.648593\n",
      "Train Epoch: 18 [4800/33600 (14%)]\tLoss: 0.434533\n",
      "Train Epoch: 18 [6400/33600 (19%)]\tLoss: 0.776298\n",
      "Train Epoch: 18 [8000/33600 (24%)]\tLoss: 0.592639\n",
      "Train Epoch: 18 [9600/33600 (29%)]\tLoss: 0.466096\n",
      "Train Epoch: 18 [11200/33600 (33%)]\tLoss: 0.733287\n",
      "Train Epoch: 18 [12800/33600 (38%)]\tLoss: 0.958184\n",
      "Train Epoch: 18 [14400/33600 (43%)]\tLoss: 0.698927\n",
      "Train Epoch: 18 [16000/33600 (48%)]\tLoss: 0.824200\n",
      "Train Epoch: 18 [17600/33600 (52%)]\tLoss: 0.540633\n",
      "Train Epoch: 18 [19200/33600 (57%)]\tLoss: 0.604306\n",
      "Train Epoch: 18 [20800/33600 (62%)]\tLoss: 0.774494\n",
      "Train Epoch: 18 [22400/33600 (67%)]\tLoss: 0.894885\n",
      "Train Epoch: 18 [24000/33600 (71%)]\tLoss: 0.503770\n",
      "Train Epoch: 18 [25600/33600 (76%)]\tLoss: 0.455570\n",
      "Train Epoch: 18 [27200/33600 (81%)]\tLoss: 1.004169\n",
      "Train Epoch: 18 [28800/33600 (86%)]\tLoss: 0.849717\n",
      "Train Epoch: 18 [30400/33600 (90%)]\tLoss: 0.723941\n",
      "Train Epoch: 18 [32000/33600 (95%)]\tLoss: 0.930842\n",
      "Train Epoch: 18 [33600/33600 (100%)]\tLoss: 0.988853\n",
      "\n",
      "Average Val Loss: 0.3004, Val Accuracy: 7836/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 19 [1600/33600 (5%)]\tLoss: 0.316341\n",
      "Train Epoch: 19 [3200/33600 (10%)]\tLoss: 0.525954\n",
      "Train Epoch: 19 [4800/33600 (14%)]\tLoss: 0.433113\n",
      "Train Epoch: 19 [6400/33600 (19%)]\tLoss: 0.527055\n",
      "Train Epoch: 19 [8000/33600 (24%)]\tLoss: 0.629400\n",
      "Train Epoch: 19 [9600/33600 (29%)]\tLoss: 0.461451\n",
      "Train Epoch: 19 [11200/33600 (33%)]\tLoss: 0.430755\n",
      "Train Epoch: 19 [12800/33600 (38%)]\tLoss: 0.903901\n",
      "Train Epoch: 19 [14400/33600 (43%)]\tLoss: 0.713437\n",
      "Train Epoch: 19 [16000/33600 (48%)]\tLoss: 0.461245\n",
      "Train Epoch: 19 [17600/33600 (52%)]\tLoss: 0.504662\n",
      "Train Epoch: 19 [19200/33600 (57%)]\tLoss: 0.921148\n",
      "Train Epoch: 19 [20800/33600 (62%)]\tLoss: 0.454971\n",
      "Train Epoch: 19 [22400/33600 (67%)]\tLoss: 0.930586\n",
      "Train Epoch: 19 [24000/33600 (71%)]\tLoss: 0.525160\n",
      "Train Epoch: 19 [25600/33600 (76%)]\tLoss: 0.637579\n",
      "Train Epoch: 19 [27200/33600 (81%)]\tLoss: 0.653888\n",
      "Train Epoch: 19 [28800/33600 (86%)]\tLoss: 0.565417\n",
      "Train Epoch: 19 [30400/33600 (90%)]\tLoss: 0.858337\n",
      "Train Epoch: 19 [32000/33600 (95%)]\tLoss: 0.344443\n",
      "Train Epoch: 19 [33600/33600 (100%)]\tLoss: 0.643137\n",
      "\n",
      "Average Val Loss: 0.3049, Val Accuracy: 7832/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 20 [1600/33600 (5%)]\tLoss: 0.357218\n",
      "Train Epoch: 20 [3200/33600 (10%)]\tLoss: 1.237354\n",
      "Train Epoch: 20 [4800/33600 (14%)]\tLoss: 0.557104\n",
      "Train Epoch: 20 [6400/33600 (19%)]\tLoss: 0.490751\n",
      "Train Epoch: 20 [8000/33600 (24%)]\tLoss: 0.782126\n",
      "Train Epoch: 20 [9600/33600 (29%)]\tLoss: 0.580619\n",
      "Train Epoch: 20 [11200/33600 (33%)]\tLoss: 1.181402\n",
      "Train Epoch: 20 [12800/33600 (38%)]\tLoss: 0.663208\n",
      "Train Epoch: 20 [14400/33600 (43%)]\tLoss: 1.212750\n",
      "Train Epoch: 20 [16000/33600 (48%)]\tLoss: 0.339613\n",
      "Train Epoch: 20 [17600/33600 (52%)]\tLoss: 0.721721\n",
      "Train Epoch: 20 [19200/33600 (57%)]\tLoss: 0.738538\n",
      "Train Epoch: 20 [20800/33600 (62%)]\tLoss: 0.342290\n",
      "Train Epoch: 20 [22400/33600 (67%)]\tLoss: 0.524280\n",
      "Train Epoch: 20 [24000/33600 (71%)]\tLoss: 0.516409\n",
      "Train Epoch: 20 [25600/33600 (76%)]\tLoss: 0.618410\n",
      "Train Epoch: 20 [27200/33600 (81%)]\tLoss: 0.398536\n",
      "Train Epoch: 20 [28800/33600 (86%)]\tLoss: 0.607933\n",
      "Train Epoch: 20 [30400/33600 (90%)]\tLoss: 0.413446\n",
      "Train Epoch: 20 [32000/33600 (95%)]\tLoss: 0.369754\n",
      "Train Epoch: 20 [33600/33600 (100%)]\tLoss: 1.066629\n",
      "\n",
      "Average Val Loss: 0.2883, Val Accuracy: 7879/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 21 [1600/33600 (5%)]\tLoss: 0.351068\n",
      "Train Epoch: 21 [3200/33600 (10%)]\tLoss: 0.721278\n",
      "Train Epoch: 21 [4800/33600 (14%)]\tLoss: 0.560636\n",
      "Train Epoch: 21 [6400/33600 (19%)]\tLoss: 0.624063\n",
      "Train Epoch: 21 [8000/33600 (24%)]\tLoss: 0.601440\n",
      "Train Epoch: 21 [9600/33600 (29%)]\tLoss: 0.621113\n",
      "Train Epoch: 21 [11200/33600 (33%)]\tLoss: 0.433371\n",
      "Train Epoch: 21 [12800/33600 (38%)]\tLoss: 0.592454\n",
      "Train Epoch: 21 [14400/33600 (43%)]\tLoss: 0.476066\n",
      "Train Epoch: 21 [16000/33600 (48%)]\tLoss: 0.774434\n",
      "Train Epoch: 21 [17600/33600 (52%)]\tLoss: 0.545897\n",
      "Train Epoch: 21 [19200/33600 (57%)]\tLoss: 0.365441\n",
      "Train Epoch: 21 [20800/33600 (62%)]\tLoss: 0.908890\n",
      "Train Epoch: 21 [22400/33600 (67%)]\tLoss: 0.704960\n",
      "Train Epoch: 21 [24000/33600 (71%)]\tLoss: 0.786050\n",
      "Train Epoch: 21 [25600/33600 (76%)]\tLoss: 0.865046\n",
      "Train Epoch: 21 [27200/33600 (81%)]\tLoss: 0.638747\n",
      "Train Epoch: 21 [28800/33600 (86%)]\tLoss: 0.958704\n",
      "Train Epoch: 21 [30400/33600 (90%)]\tLoss: 1.223951\n",
      "Train Epoch: 21 [32000/33600 (95%)]\tLoss: 0.483394\n",
      "Train Epoch: 21 [33600/33600 (100%)]\tLoss: 0.480125\n",
      "\n",
      "Average Val Loss: 0.3030, Val Accuracy: 7855/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 22 [1600/33600 (5%)]\tLoss: 0.577607\n",
      "Train Epoch: 22 [3200/33600 (10%)]\tLoss: 0.846783\n",
      "Train Epoch: 22 [4800/33600 (14%)]\tLoss: 0.640493\n",
      "Train Epoch: 22 [6400/33600 (19%)]\tLoss: 0.474399\n",
      "Train Epoch: 22 [8000/33600 (24%)]\tLoss: 0.538857\n",
      "Train Epoch: 22 [9600/33600 (29%)]\tLoss: 0.576914\n",
      "Train Epoch: 22 [11200/33600 (33%)]\tLoss: 0.816002\n",
      "Train Epoch: 22 [12800/33600 (38%)]\tLoss: 0.289743\n",
      "Train Epoch: 22 [14400/33600 (43%)]\tLoss: 0.312507\n",
      "Train Epoch: 22 [16000/33600 (48%)]\tLoss: 0.584889\n",
      "Train Epoch: 22 [17600/33600 (52%)]\tLoss: 0.700547\n",
      "Train Epoch: 22 [19200/33600 (57%)]\tLoss: 0.964932\n",
      "Train Epoch: 22 [20800/33600 (62%)]\tLoss: 0.945833\n",
      "Train Epoch: 22 [22400/33600 (67%)]\tLoss: 0.686006\n",
      "Train Epoch: 22 [24000/33600 (71%)]\tLoss: 0.982366\n",
      "Train Epoch: 22 [25600/33600 (76%)]\tLoss: 0.661022\n",
      "Train Epoch: 22 [27200/33600 (81%)]\tLoss: 0.725786\n",
      "Train Epoch: 22 [28800/33600 (86%)]\tLoss: 0.588187\n",
      "Train Epoch: 22 [30400/33600 (90%)]\tLoss: 0.521712\n",
      "Train Epoch: 22 [32000/33600 (95%)]\tLoss: 0.892151\n",
      "Train Epoch: 22 [33600/33600 (100%)]\tLoss: 0.650218\n",
      "\n",
      "Average Val Loss: 0.3035, Val Accuracy: 7848/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 23 [1600/33600 (5%)]\tLoss: 0.352468\n",
      "Train Epoch: 23 [3200/33600 (10%)]\tLoss: 0.665306\n",
      "Train Epoch: 23 [4800/33600 (14%)]\tLoss: 0.864121\n",
      "Train Epoch: 23 [6400/33600 (19%)]\tLoss: 0.456542\n",
      "Train Epoch: 23 [8000/33600 (24%)]\tLoss: 0.449087\n",
      "Train Epoch: 23 [9600/33600 (29%)]\tLoss: 0.741348\n",
      "Train Epoch: 23 [11200/33600 (33%)]\tLoss: 0.584934\n",
      "Train Epoch: 23 [12800/33600 (38%)]\tLoss: 0.751781\n",
      "Train Epoch: 23 [14400/33600 (43%)]\tLoss: 0.471620\n",
      "Train Epoch: 23 [16000/33600 (48%)]\tLoss: 0.849036\n",
      "Train Epoch: 23 [17600/33600 (52%)]\tLoss: 0.624818\n",
      "Train Epoch: 23 [19200/33600 (57%)]\tLoss: 0.467223\n",
      "Train Epoch: 23 [20800/33600 (62%)]\tLoss: 0.488369\n",
      "Train Epoch: 23 [22400/33600 (67%)]\tLoss: 0.564187\n",
      "Train Epoch: 23 [24000/33600 (71%)]\tLoss: 0.917373\n",
      "Train Epoch: 23 [25600/33600 (76%)]\tLoss: 0.415728\n",
      "Train Epoch: 23 [27200/33600 (81%)]\tLoss: 0.418047\n",
      "Train Epoch: 23 [28800/33600 (86%)]\tLoss: 0.581132\n",
      "Train Epoch: 23 [30400/33600 (90%)]\tLoss: 0.498793\n",
      "Train Epoch: 23 [32000/33600 (95%)]\tLoss: 1.109166\n",
      "Train Epoch: 23 [33600/33600 (100%)]\tLoss: 0.305949\n",
      "\n",
      "Average Val Loss: 0.3012, Val Accuracy: 7861/8400 (93.000%)\n",
      "\n",
      "Train Epoch: 24 [1600/33600 (5%)]\tLoss: 1.095853\n",
      "Train Epoch: 24 [3200/33600 (10%)]\tLoss: 0.576421\n",
      "Train Epoch: 24 [4800/33600 (14%)]\tLoss: 0.423135\n",
      "Train Epoch: 24 [6400/33600 (19%)]\tLoss: 0.501167\n",
      "Train Epoch: 24 [8000/33600 (24%)]\tLoss: 0.546470\n",
      "Train Epoch: 24 [9600/33600 (29%)]\tLoss: 0.646795\n",
      "Train Epoch: 24 [11200/33600 (33%)]\tLoss: 0.582877\n",
      "Train Epoch: 24 [12800/33600 (38%)]\tLoss: 0.683825\n",
      "Train Epoch: 24 [14400/33600 (43%)]\tLoss: 0.651183\n",
      "Train Epoch: 24 [16000/33600 (48%)]\tLoss: 0.568768\n",
      "Train Epoch: 24 [17600/33600 (52%)]\tLoss: 0.843548\n",
      "Train Epoch: 24 [19200/33600 (57%)]\tLoss: 0.549766\n",
      "Train Epoch: 24 [20800/33600 (62%)]\tLoss: 0.551790\n",
      "Train Epoch: 24 [22400/33600 (67%)]\tLoss: 0.786761\n",
      "Train Epoch: 24 [24000/33600 (71%)]\tLoss: 0.507338\n",
      "Train Epoch: 24 [25600/33600 (76%)]\tLoss: 0.511240\n",
      "Train Epoch: 24 [27200/33600 (81%)]\tLoss: 0.607689\n",
      "Train Epoch: 24 [28800/33600 (86%)]\tLoss: 0.848304\n",
      "Train Epoch: 24 [30400/33600 (90%)]\tLoss: 0.653595\n",
      "Train Epoch: 24 [32000/33600 (95%)]\tLoss: 0.868632\n",
      "Train Epoch: 24 [33600/33600 (100%)]\tLoss: 0.648157\n",
      "\n",
      "Average Val Loss: 0.2983, Val Accuracy: 7880/8400 (93.000%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    train_model(n)\n",
    "    evaluate(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # input channel = 1, output channel = 6, kernel_size = 5\n",
    "        # input size = (32, 32), output size = (28, 28)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # input channel = 6, output channel = 16, kernel_size = 5\n",
    "        # input size = (14, 14), output size = (10, 10)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # input dim = 16*5*5, output dim = 120\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # input dim = 120, output dim = 84\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # input dim = 84, output dim = 10\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pool size = 2\n",
    "        # input size = (28, 28), output size = (14, 14), output channel = 6\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # pool size = 2\n",
    "        # input size = (10, 10), output size = (5, 5), output channel = 16\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten as one dimension\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # input dim = 16*5*5, output dim = 120\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # input dim = 120, output dim = 84\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # input dim = 84, output dim = 10\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "model = model.double()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.DoubleTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-0d9b67836836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-602cf5e56c5a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# pool size = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# input size = (28, 28), output size = (14, 14), output channel = 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# pool size = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# input size = (10, 10), output size = (5, 5), output channel = 16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.DoubleTensor) should be the same"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "train_losses, test_losses = [] ,[]\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images,labels in train_loader:\n",
    "        train = Variable(images.view(-1,1,28,28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(train)\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad(): #Turning off gradients to speed up\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                \n",
    "                test = Variable(images.view(-1,1,28,28))\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "                log_ps = model(test)\n",
    "                test_loss += criterion(log_ps,labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim = 1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()        \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
